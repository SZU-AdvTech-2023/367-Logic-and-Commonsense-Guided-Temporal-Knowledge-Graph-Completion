{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 7122401,
     "sourceType": "datasetVersion",
     "datasetId": 4108297
    }
   ],
   "dockerImageVersionId": 30588,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import math\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Tuple, List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class TKBCModel(nn.Module, ABC):\n",
    "    @abstractmethod\n",
    "    def get_rhs(self, chunk_begin: int, chunk_size: int):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_queries(self, queries: torch.Tensor):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_rhs_static(self, queries: torch.Tensor):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def score(self, x: torch.Tensor):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward_over_time(self, x: torch.Tensor):\n",
    "        pass\n",
    "\n",
    "    def get_ranking(\n",
    "            self, queries: torch.Tensor,\n",
    "            filters: Dict[Tuple[int, int, int], List[int]],\n",
    "            batch_size: int = 1000, chunk_size: int = -1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Returns filtered ranking for each queries.\n",
    "        :param queries: a torch.LongTensor of quadruples (lhs, rel, rhs, timestamp)\n",
    "        :param filters: filters[(lhs, rel, ts)] gives the elements to filter from ranking\n",
    "        :param batch_size: maximum number of queries processed at once\n",
    "        :param chunk_size: maximum number of candidates processed at once\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if chunk_size < 0:\n",
    "            chunk_size = self.sizes[2]\n",
    "        ranks = torch.ones(len(queries))\n",
    "        with torch.no_grad():\n",
    "            c_begin = 0\n",
    "            while c_begin < self.sizes[2]:\n",
    "                b_begin = 0\n",
    "                rhs = self.get_rhs(c_begin, chunk_size)\n",
    "                while b_begin < len(queries):\n",
    "                    these_queries = queries[b_begin:b_begin + batch_size]\n",
    "                    q = self.get_queries(these_queries)\n",
    "\n",
    "                    # scores = q[0] @ rhs + 0.1 * q[1] @ self.get_rhs_static(c_begin, chunk_size)\n",
    "                    # targets = self.score(these_queries)\n",
    "                    scores_tem = q[0] @ rhs\n",
    "                    scores_cs = q[1] @ self.get_rhs_static(c_begin, chunk_size)\n",
    "                    targets_tem, targets_cs = self.score(these_queries)\n",
    "                    # print(\"scores_cs:\\n{}\\n\\ntargets_cs:\\n{}\\n\".format(scores_cs, targets_cs))\n",
    "\n",
    "                    # assert not torch.any(torch.isinf(scores)), \"inf scores\"\n",
    "                    # assert not torch.any(torch.isnan(scores)), \"nan scores\"\n",
    "                    # assert not torch.any(torch.isinf(targets)), \"inf targets\"\n",
    "                    # assert not torch.any(torch.isnan(targets)), \"nan targets\"\n",
    "\n",
    "                    # set filtered and true scores to -1e6 to be ignored\n",
    "                    # take care that scores are chunked\n",
    "                    for i, query in enumerate(these_queries):\n",
    "                        filter_out = filters[(query[0].item(), query[1].item(), query[3].item())]\n",
    "                        filter_out += [queries[b_begin + i, 2].item()]\n",
    "                        if chunk_size < self.sizes[2]:\n",
    "                            filter_in_chunk = [\n",
    "                                int(x - c_begin) for x in filter_out\n",
    "                                if c_begin <= x < c_begin + chunk_size\n",
    "                            ]\n",
    "                            scores_tem[i, torch.LongTensor(filter_in_chunk)] = -1e6\n",
    "                        else:\n",
    "                            scores_tem[i, torch.LongTensor(filter_out)] = -1e6\n",
    "                    ranks[b_begin:b_begin + batch_size] += torch.sum(\n",
    "                        (torch.mul(scores_tem >= targets_tem, scores_cs > targets_cs)).float(), dim=1\n",
    "                    ).cpu()\n",
    "\n",
    "                    b_begin += batch_size\n",
    "\n",
    "                c_begin += chunk_size\n",
    "        return ranks\n",
    "\n",
    "    def get_auc(\n",
    "            self, queries: torch.Tensor, batch_size: int = 1000\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Returns filtered ranking for each queries.\n",
    "        :param queries: a torch.LongTensor of quadruples (lhs, rel, rhs, begin, end)\n",
    "        :param batch_size: maximum number of queries processed at once\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        all_scores, all_truth = [], []\n",
    "        all_ts_ids = None\n",
    "        with torch.no_grad():\n",
    "            b_begin = 0\n",
    "            while b_begin < len(queries):\n",
    "                these_queries = queries[b_begin:b_begin + batch_size]\n",
    "                scores = self.forward_over_time(these_queries)\n",
    "                all_scores.append(scores.cpu().numpy())\n",
    "                if all_ts_ids is None:\n",
    "                    all_ts_ids = torch.arange(0, scores.shape[1]).cuda()[None, :]\n",
    "                assert not torch.any(torch.isinf(scores) + torch.isnan(scores)), \"inf or nan scores\"\n",
    "                truth = (all_ts_ids <= these_queries[:, 4][:, None]) * (all_ts_ids >= these_queries[:, 3][:, None])\n",
    "                all_truth.append(truth.cpu().numpy())\n",
    "                b_begin += batch_size\n",
    "\n",
    "        return np.concatenate(all_truth), np.concatenate(all_scores)\n",
    "\n",
    "    def get_time_ranking(\n",
    "            self, queries: torch.Tensor, filters: List[List[int]], chunk_size: int = -1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Returns filtered ranking for a batch of queries ordered by timestamp.\n",
    "        :param queries: a torch.LongTensor of quadruples (lhs, rel, rhs, timestamp)\n",
    "        :param filters: ordered filters\n",
    "        :param chunk_size: maximum number of candidates processed at once\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if chunk_size < 0:\n",
    "            chunk_size = self.sizes[2]\n",
    "        ranks = torch.ones(len(queries))\n",
    "        with torch.no_grad():\n",
    "            c_begin = 0\n",
    "            q = self.get_queries(queries)\n",
    "            targets = self.score(queries)\n",
    "            while c_begin < self.sizes[2]:\n",
    "                rhs = self.get_rhs(c_begin, chunk_size)\n",
    "                scores = q @ rhs\n",
    "                # set filtered and true scores to -1e6 to be ignored\n",
    "                # take care that scores are chunked\n",
    "                for i, (query, filter) in enumerate(zip(queries, filters)):\n",
    "                    filter_out = filter + [query[2].item()]\n",
    "                    if chunk_size < self.sizes[2]:\n",
    "                        filter_in_chunk = [\n",
    "                            int(x - c_begin) for x in filter_out\n",
    "                            if c_begin <= x < c_begin + chunk_size\n",
    "                        ]\n",
    "                        max_to_filter = max(filter_in_chunk + [-1])\n",
    "                        assert max_to_filter < scores.shape[1], f\"fuck {scores.shape[1]} {max_to_filter}\"\n",
    "                        scores[i, filter_in_chunk] = -1e6\n",
    "                    else:\n",
    "                        scores[i, filter_out] = -1e6\n",
    "                ranks += torch.sum(\n",
    "                    (scores >= targets).float(), dim=1\n",
    "                ).cpu()\n",
    "\n",
    "                c_begin += chunk_size\n",
    "        return ranks\n",
    "\n",
    "\n",
    "class LCGE(TKBCModel):\n",
    "    def __init__(\n",
    "            self, sizes: Tuple[int, int, int, int], rank: int, Rules, w_static,\n",
    "            no_time_emb=False, init_size: float = 1e-3\n",
    "    ):\n",
    "        super(LCGE, self).__init__()\n",
    "        self.sizes = sizes\n",
    "        self.rank = rank\n",
    "        self.rank_static = rank // 20\n",
    "        self.w_static = w_static\n",
    "\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(s, 2 * rank, sparse=True)\n",
    "            for s in [sizes[0], sizes[1], sizes[3], sizes[1], 1]  # last embedding modules contains no_time embeddings\n",
    "        ])\n",
    "        self.static_embeddings = nn.ModuleList([\n",
    "            nn.Embedding(s, 2 * self.rank_static, sparse=True)\n",
    "            for s in [sizes[0], sizes[1]]\n",
    "        ])\n",
    "        \n",
    "        self.self_attention = nn.MultiheadAttention(embed_dim=2 * rank, num_heads=2)  # 多头自注意力机制\n",
    "        self.self_attention_s = nn.MultiheadAttention(embed_dim=2 * self.rank_static, num_heads=2)  # 多头自注意力机制\n",
    "        \n",
    "        self.embeddings[0].weight.data *= init_size\n",
    "        self.embeddings[1].weight.data *= init_size\n",
    "        self.embeddings[2].weight.data *= init_size\n",
    "        self.embeddings[3].weight.data *= init_size\n",
    "        self.embeddings[4].weight.data *= init_size  # time transition\n",
    "        self.static_embeddings[0].weight.data *= init_size  # static entity embedding\n",
    "        self.static_embeddings[1].weight.data *= init_size  # static relation embedding\n",
    "        \n",
    "\n",
    "        self.no_time_emb = no_time_emb\n",
    "        # self.rule1_p1, self.rule1_p2, self.rule2_p1, self.rule2_p2, self.rule2_p3, self.rule2_p4 = Rules\n",
    "        self.rule1_p1, self.rule1_p2, self.rule2_p1, self.rule2_p2, self.rule2_p3, self.rule2_p4 = Rules\n",
    "\n",
    "    @staticmethod\n",
    "    def has_time():\n",
    "        return True\n",
    "\n",
    "    def score(self, x):\n",
    "#         print(x.shape)\n",
    "#         print(x[:5])\n",
    "        lhs = self.embeddings[0](x[:, 0])\n",
    "        rel = self.embeddings[1](x[:, 1])\n",
    "        rel_no_time = self.embeddings[3](x[:, 1])\n",
    "        rhs = self.embeddings[0](x[:, 2])\n",
    "        time = self.embeddings[2](x[:, 3])\n",
    "        input_sequence = torch.cat([lhs, rel, rhs,time], dim=0)  \n",
    "        # 将左实体、关系、右实体拼接成一个序列\n",
    "        attended_output, _ = self.self_attention(input_sequence, input_sequence, input_sequence)\n",
    "        \n",
    "        lhs, rel, rhs,time = torch.split(attended_output, [lhs.size(0), rel.size(0), rhs.size(0),time.size(0)], dim=0)\n",
    "        \n",
    "\n",
    "        lhs = lhs[:, :self.rank], lhs[:, self.rank:]\n",
    "        rel = rel[:, :self.rank], rel[:, self.rank:]\n",
    "        rhs = rhs[:, :self.rank], rhs[:, self.rank:]\n",
    "        time = time[:, :self.rank], time[:, self.rank:]\n",
    "        rnt = rel_no_time[:, :self.rank], rel_no_time[:, self.rank:]\n",
    "\n",
    "        rt = rel[0] * time[0], rel[1] * time[0], rel[0] * time[1], rel[1] * time[1]\n",
    "        full_rel = (rt[0] - rt[3]) + rnt[0], (rt[1] + rt[2]) + rnt[1]\n",
    "\n",
    "        h_static = self.static_embeddings[0](x[:, 0])\n",
    "        r_static = self.static_embeddings[1](x[:, 1])\n",
    "        t_static = self.static_embeddings[0](x[:, 2])\n",
    "        \n",
    "        input_sequence_static = torch.cat([h_static, r_static, t_static], dim=0)  \n",
    "        # 将左实体、关系、右实体拼接成一个序列\n",
    "        attended_output_static, _ = self.self_attention_s(input_sequence_static, input_sequence_static, input_sequence_static)\n",
    "        \n",
    "        h_static, r_static, t_static = torch.split(attended_output_static, [h_static.size(0), r_static.size(0), t_static.size(0)], dim=0)\n",
    "\n",
    "\n",
    "        h_static = h_static[:, :self.rank_static], h_static[:, self.rank_static:]\n",
    "        r_static = r_static[:, :self.rank_static], r_static[:, self.rank_static:]\n",
    "        t_static = t_static[:, :self.rank_static], t_static[:, self.rank_static:]\n",
    "        # print(\"h size:{}\\tr size:{}\\ttsize:{}\".format(h_static[0].shape, r_static[0].shape, t_static[0].shape))\n",
    "\n",
    "        return torch.sum(\n",
    "            (lhs[0] * full_rel[0] - lhs[1] * full_rel[1]) * rhs[0] +\n",
    "            (lhs[1] * full_rel[0] + lhs[0] * full_rel[1]) * rhs[1],\n",
    "            1, keepdim=True\n",
    "        ), torch.sum(\n",
    "            (h_static[0] * r_static[0] - h_static[1] * r_static[1]) * t_static[0] +\n",
    "            (h_static[1] * r_static[0] + h_static[0] * r_static[1]) * t_static[1],\n",
    "            1, keepdim=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        lhs = self.embeddings[0](x[:, 0])\n",
    "        rel = self.embeddings[1](x[:, 1])\n",
    "        rel_no_time = self.embeddings[3](x[:, 1])\n",
    "        rhs = self.embeddings[0](x[:, 2])\n",
    "        time = self.embeddings[2](x[:, 3])\n",
    "        transt = self.embeddings[4](torch.LongTensor([0]).cuda())\n",
    "        # 提取输入数据的embedding：\n",
    "        # lhs、rel、rhs、time和transt分别提取了左实体、关系、右实体、时间和时间转移的embedding。\n",
    "        # 这些embedding是通过调用nn.Embedding层得到的。\n",
    "        # print('lhs:', lhs.shape, '\\n', 'rel:', rel.shape, '\\n', 'rhs:', rhs.shape, '\\n', 'tiem:', time.shape)\n",
    "        # embedding后是(1000,4000)   1000为epoch的限制，每个单词转化为4000维的向量\n",
    "\n",
    "        input_sequence = torch.cat([lhs, rel, rhs,time], dim=0)  \n",
    "        # 将左实体、关系、右实体拼接成一个序列\n",
    "        attended_output, _ = self.self_attention(input_sequence, input_sequence, input_sequence)\n",
    "        \n",
    "        lhs, rel, rhs,time = torch.split(attended_output, [lhs.size(0), rel.size(0), rhs.size(0),time.size(0)], dim=0)\n",
    "        \n",
    "        \n",
    "        lhs = lhs[:, :self.rank], lhs[:, self.rank:]\n",
    "        rel = rel[:, :self.rank], rel[:, self.rank:]\n",
    "        rhs = rhs[:, :self.rank], rhs[:, self.rank:]\n",
    "        time = time[:, :self.rank], time[:, self.rank:]\n",
    "        transt = transt[:, :self.rank], transt[:, self.rank:]\n",
    "        # print(transt[1])\n",
    "        # print('lhs[0]:', lhs[0].shape, '\\n', 'rel[0]:', rel[0].shape, '\\n', 'rhs[0]:', rhs[0].shape, '\\n', 'tiem[0]:', time[0].shape)\n",
    "\n",
    "        rnt = rel_no_time[:, :self.rank], rel_no_time[:, self.rank:]\n",
    "\n",
    "        right = self.embeddings[0].weight\n",
    "        right = right[:, :self.rank], right[:, self.rank:]\n",
    "        # 对提取的embedding进行分割，将每个embedding向量按rank分成两部分，前后各一半\n",
    "        # 分割为两部分的原因是因为要将向量映射为实部和虚部，为了使用复平面进行运算\n",
    "\n",
    "        rt = rel[0] * time[0], rel[1] * time[0], rel[0] * time[1], rel[1] * time[1]\n",
    "        rrt = rt[0] - rt[3], rt[1] + rt[2]\n",
    "        full_rel = rrt[0] + rnt[0], rrt[1] + rnt[1]\n",
    "\n",
    "\n",
    "        h_static = self.static_embeddings[0](x[:, 0])\n",
    "        r_static = self.static_embeddings[1](x[:, 1])\n",
    "        t_static = self.static_embeddings[0](x[:, 2])\n",
    "        \n",
    "        input_sequence_static = torch.cat([h_static, r_static, t_static], dim=0)  \n",
    "        # 将左实体、关系、右实体拼接成一个序列\n",
    "        attended_output_static, _ = self.self_attention_s(input_sequence_static, input_sequence_static, input_sequence_static)\n",
    "        \n",
    "        h_static, r_static, t_static = torch.split(attended_output_static, [h_static.size(0), r_static.size(0), t_static.size(0)], dim=0)\n",
    "\n",
    "        h_static = h_static[:, :self.rank_static], h_static[:, self.rank_static:]\n",
    "        r_static = r_static[:, :self.rank_static], r_static[:, self.rank_static:]\n",
    "        t_static = t_static[:, :self.rank_static], t_static[:, self.rank_static:]\n",
    "        # 静态，无time值的情况下的embedding处理\n",
    "        right_static = self.static_embeddings[0].weight\n",
    "        right_static = right_static[:, :self.rank_static], right_static[:, self.rank_static:]\n",
    "\n",
    "        regularizer = (\n",
    "            math.pow(2, 1 / 3) * torch.sqrt(lhs[0] ** 2 + lhs[1] ** 2),\n",
    "            torch.sqrt(rrt[0] ** 2 + rrt[1] ** 2),\n",
    "            torch.sqrt(rnt[0] ** 2 + rnt[1] ** 2),\n",
    "            math.pow(2, 1 / 3) * torch.sqrt(rhs[0] ** 2 + rhs[1] ** 2),\n",
    "            torch.sqrt(h_static[0] ** 2 + h_static[1] ** 2),\n",
    "            torch.sqrt(r_static[0] ** 2 + r_static[1] ** 2),\n",
    "            torch.sqrt(t_static[0] ** 2 + t_static[1] ** 2)\n",
    "        )\n",
    "\n",
    "        rule = 0.\n",
    "        rule_num = 0\n",
    "        for rel_1 in x[:, 1]:\n",
    "            rel_1_str = str(rel_1.item())\n",
    "            if rel_1_str in self.rule1_p2:\n",
    "                rel1_emb = self.embeddings[3](rel_1)\n",
    "                for rel_2 in self.rule1_p2[rel_1_str]:\n",
    "                    weight_r = self.rule1_p2[rel_1_str][rel_2]\n",
    "                    rel2_emb = self.embeddings[3](torch.LongTensor([int(rel_2)]).cuda())[0]\n",
    "                    rule += weight_r * torch.sum(torch.abs(rel1_emb - rel2_emb) ** 3)\n",
    "                    rule_num += 1\n",
    "\n",
    "        for rel_1 in x[:, 1]:\n",
    "            rel_1_str = str(rel_1.item())\n",
    "            if rel_1_str in self.rule1_p2:\n",
    "                rel1_emb = self.embeddings[3](rel_1)\n",
    "                rel1_split = rel1_emb[:self.rank], rel1_emb[self.rank:]\n",
    "                for rel_2 in self.rule1_p2[rel_1_str]:\n",
    "                    weight_r = self.rule1_p2[rel_1_str][rel_2]\n",
    "                    rel2_emb = self.embeddings[3](torch.LongTensor([int(rel_2)]).cuda())[0]\n",
    "                    rel2_split = rel2_emb[:self.rank], rel2_emb[self.rank:]\n",
    "                    tt = rel2_split[0] * transt[0][0], rel2_split[1] * transt[0][0], rel2_split[0] * transt[1][0], \\\n",
    "                         rel2_split[1] * transt[1][0]\n",
    "                    rtt = tt[0] - tt[3], tt[1] + tt[2]\n",
    "                    # print(\"rel1_split:\\t\", rel1_split[0])\n",
    "                    rule += weight_r * (torch.sum(torch.abs(rel1_split[0] - rtt[0]) ** 3) + torch.sum(\n",
    "                        torch.abs(rel1_split[1] - rtt[1]) ** 3))\n",
    "                    rule_num += 1\n",
    "\n",
    "        for rel_1 in x[:, 1]:\n",
    "            if rel_1 in self.rule2_p1:\n",
    "                rel1_emb = self.embeddings[3](rel_1)\n",
    "                rel1_split = rel1_emb[:self.rank], rel1_emb[self.rank:]\n",
    "                for body in self.rule2_p1[rel_1]:\n",
    "                    rel_2, rel_3 = body\n",
    "                    weight_r = self.rule2_p1[rel_1][body]\n",
    "                    rel2_emb = self.embeddings[3](torch.LongTensor([rel_2]).cuda())[0]\n",
    "                    rel3_emb = self.embeddings[3](torch.LongTensor([rel_3]).cuda())[0]\n",
    "                    rel2_split = rel2_emb[:self.rank], rel2_emb[self.rank:]\n",
    "                    rel3_split = rel3_emb[:self.rank], rel3_emb[self.rank:]\n",
    "                    tt2 = rel2_split[0] * transt[0][0], rel2_split[1] * transt[0][0], rel2_split[0] * transt[1][0], \\\n",
    "                          rel2_split[1] * transt[1][0]\n",
    "                    rtt2 = tt2[0] - tt2[3], tt2[1] + tt2[2]\n",
    "                    ttt2 = rtt2[0] * transt[0][0], rtt2[1] * transt[0][0], rtt2[0] * transt[1][0], rtt2[1] * transt[1][\n",
    "                        0]\n",
    "                    rttt2 = ttt2[0] - ttt2[3], ttt2[1] + ttt2[2]\n",
    "                    tt3 = rel3_split[0] * transt[0][0], rel3_split[1] * transt[0][0], rel3_split[0] * transt[1][0], \\\n",
    "                          rel3_split[1] * transt[1][0]\n",
    "                    rtt3 = tt3[0] - tt3[3], tt3[1] + tt3[2]\n",
    "                    tt = rtt3[0] * rttt2[0], rtt3[1] * rttt2[0], rtt3[0] * rttt2[1], rtt3[1] * rttt2[1]\n",
    "                    rtt = tt[0] - tt[3], tt[1] + tt[2]\n",
    "                    # print(\"rel1_split:\\t\", rel1_split[0])\n",
    "                    rule += weight_r * (torch.sum(torch.abs(rel1_split[0] - rtt[0]) ** 3) + torch.sum(\n",
    "                        torch.abs(rel1_split[1] - rtt[1]) ** 3))\n",
    "                    rule_num += 1\n",
    "\n",
    "        for rel_1 in x[:, 1]:\n",
    "            if rel_1 in self.rule2_p2:\n",
    "                rel1_emb = self.embeddings[3](rel_1)\n",
    "                rel1_split = rel1_emb[:self.rank], rel1_emb[self.rank:]\n",
    "                for body in self.rule2_p2[rel_1]:\n",
    "                    rel_2, rel_3 = body\n",
    "                    weight_r = self.rule2_p2[rel_1][body]\n",
    "                    rel2_emb = self.embeddings[3](torch.LongTensor([rel_2]).cuda())[0]\n",
    "                    rel3_emb = self.embeddings[3](torch.LongTensor([rel_3]).cuda())[0]\n",
    "                    rel2_split = rel2_emb[:self.rank], rel2_emb[self.rank:]\n",
    "                    rel3_split = rel3_emb[:self.rank], rel3_emb[self.rank:]\n",
    "                    tt2 = rel2_split[0] * transt[0][0], rel2_split[1] * transt[0][0], rel2_split[0] * transt[1][0], \\\n",
    "                          rel2_split[1] * transt[1][0]\n",
    "                    rtt2 = tt2[0] - tt2[3], tt2[1] + tt2[2]\n",
    "                    tt3 = rel3_split[0] * transt[0][0], rel3_split[1] * transt[0][0], rel3_split[0] * transt[1][0], \\\n",
    "                          rel3_split[1] * transt[1][0]\n",
    "                    rtt3 = tt3[0] - tt3[3], tt3[1] + tt3[2]\n",
    "                    tt = rtt3[0] * rtt2[0], rtt3[1] * rtt2[0], rtt3[0] * rtt2[1], rtt3[1] * rtt2[1]\n",
    "                    rtt = tt[0] - tt[3], tt[1] + tt[2]\n",
    "                    # print(\"rel1_split:\\t\", rel1_split[0])\n",
    "                    rule += weight_r * (torch.sum(torch.abs(rel1_split[0] - rtt[0]) ** 3) + torch.sum(\n",
    "                        torch.abs(rel1_split[1] - rtt[1]) ** 3))\n",
    "                    rule_num += 1\n",
    "\n",
    "        for rel_1 in x[:, 1]:\n",
    "            if rel_1 in self.rule2_p3:\n",
    "                rel1_emb = self.embeddings[3](rel_1)\n",
    "                rel1_split = rel1_emb[:self.rank], rel1_emb[self.rank:]\n",
    "                for body in self.rule2_p3[rel_1]:\n",
    "                    rel_2, rel_3 = body\n",
    "                    weight_r = self.rule2_p3[rel_1][body]\n",
    "                    rel2_emb = self.embeddings[3](torch.LongTensor([rel_2]).cuda())[0]\n",
    "                    rel3_emb = self.embeddings[3](torch.LongTensor([rel_3]).cuda())[0]\n",
    "                    rel2_split = rel2_emb[:self.rank], rel2_emb[self.rank:]\n",
    "                    rtt3 = rel3_emb[:self.rank], rel3_emb[self.rank:]\n",
    "                    tt2 = rel2_split[0] * transt[0][0], rel2_split[1] * transt[0][0], rel2_split[0] * transt[1][0], \\\n",
    "                          rel2_split[1] * transt[1][0]\n",
    "                    rtt2 = tt2[0] - tt2[3], tt2[1] + tt2[2]\n",
    "                    tt = rtt3[0] * rtt2[0], rtt3[1] * rtt2[0], rtt3[0] * rtt2[1], rtt3[1] * rtt2[1]\n",
    "                    rtt = tt[0] - tt[3], tt[1] + tt[2]\n",
    "                    # print(\"rel1_split:\\t\", rel1_split[0])\n",
    "                    rule += weight_r * (torch.sum(torch.abs(rel1_split[0] - rtt[0]) ** 3) + torch.sum(\n",
    "                        torch.abs(rel1_split[1] - rtt[1]) ** 3))\n",
    "                    rule_num += 1\n",
    "\n",
    "        for rel_1 in x[:, 1]:\n",
    "            if rel_1 in self.rule2_p4:\n",
    "                rel1_emb = self.embeddings[3](rel_1)\n",
    "                rel1_split = rel1_emb[:self.rank], rel1_emb[self.rank:]\n",
    "                for body in self.rule2_p4[rel_1]:\n",
    "                    rel_2, rel_3 = body\n",
    "                    weight_r = self.rule2_p4[rel_1][body]\n",
    "                    rel2_emb = self.embeddings[3](torch.LongTensor([rel_2]).cuda())[0]\n",
    "                    rel3_emb = self.embeddings[3](torch.LongTensor([rel_3]).cuda())[0]\n",
    "                    rtt2 = rel2_emb[:self.rank], rel2_emb[self.rank:]\n",
    "                    rtt3 = rel3_emb[:self.rank], rel3_emb[self.rank:]\n",
    "                    tt = rtt3[0] * rtt2[0], rtt3[1] * rtt2[0], rtt3[0] * rtt2[1], rtt3[1] * rtt2[1]\n",
    "                    rtt = tt[0] - tt[3], tt[1] + tt[2]\n",
    "                    # print(\"rel1_split:\\t\", rel1_split[0])\n",
    "                    rule += weight_r * (torch.sum(torch.abs(rel1_split[0] - rtt[0]) ** 3) + torch.sum(\n",
    "                        torch.abs(rel1_split[1] - rtt[1]) ** 3))\n",
    "                    rule_num += 1\n",
    "\n",
    "        rule = rule / rule_num\n",
    "        return (\n",
    "            (lhs[0] * full_rel[0] - lhs[1] * full_rel[1]) @ right[0].t() +\n",
    "            (lhs[1] * full_rel[0] + lhs[0] * full_rel[1]) @ right[1].t(),\n",
    "            (h_static[0] * r_static[0] - h_static[1] * r_static[1]) @ right_static[0].t() +\n",
    "            (h_static[1] * r_static[0] + h_static[0] * r_static[1]) @ right_static[1].t(),\n",
    "            regularizer,\n",
    "            self.embeddings[2].weight[:-1] if self.no_time_emb else self.embeddings[2].weight,\n",
    "            rule\n",
    "        )\n",
    "\n",
    "    def forward_over_time(self, x):\n",
    "        lhs = self.embeddings[0](x[:, 0])\n",
    "        rel = self.embeddings[1](x[:, 1])\n",
    "        rhs = self.embeddings[0](x[:, 2])\n",
    "        time = self.embeddings[2].weight\n",
    "\n",
    "        lhs = lhs[:, :self.rank], lhs[:, self.rank:]\n",
    "        rel = rel[:, :self.rank], rel[:, self.rank:]\n",
    "        rhs = rhs[:, :self.rank], rhs[:, self.rank:]\n",
    "        time = time[:, :self.rank], time[:, self.rank:]\n",
    "\n",
    "        rel_no_time = self.embeddings[3](x[:, 1])\n",
    "        rnt = rel_no_time[:, :self.rank], rel_no_time[:, self.rank:]\n",
    "\n",
    "        score_time = (\n",
    "                (lhs[0] * rel[0] * rhs[0] - lhs[1] * rel[1] * rhs[0] -\n",
    "                 lhs[1] * rel[0] * rhs[1] + lhs[0] * rel[1] * rhs[1]) @ time[0].t() +\n",
    "                (lhs[1] * rel[0] * rhs[0] - lhs[0] * rel[1] * rhs[0] +\n",
    "                 lhs[0] * rel[0] * rhs[1] - lhs[1] * rel[1] * rhs[1]) @ time[1].t()\n",
    "        )\n",
    "        base = torch.sum(\n",
    "            (lhs[0] * rnt[0] * rhs[0] - lhs[1] * rnt[1] * rhs[0] -\n",
    "             lhs[1] * rnt[0] * rhs[1] + lhs[0] * rnt[1] * rhs[1]) +\n",
    "            (lhs[1] * rnt[1] * rhs[0] - lhs[0] * rnt[0] * rhs[0] +\n",
    "             lhs[0] * rnt[1] * rhs[1] - lhs[1] * rnt[0] * rhs[1]),\n",
    "            dim=1, keepdim=True\n",
    "        )\n",
    "        return score_time + base\n",
    "\n",
    "    def get_rhs(self, chunk_begin: int, chunk_size: int):\n",
    "        return self.embeddings[0].weight.data[\n",
    "               chunk_begin:chunk_begin + chunk_size\n",
    "               ].transpose(0, 1)\n",
    "\n",
    "    def get_rhs_static(self, chunk_begin: int, chunk_size: int):\n",
    "        return self.static_embeddings[0].weight.data[\n",
    "               chunk_begin:chunk_begin + chunk_size\n",
    "               ].transpose(0, 1)\n",
    "\n",
    "    def get_queries(self, queries: torch.Tensor):\n",
    "        lhs = self.embeddings[0](queries[:, 0])\n",
    "        rel = self.embeddings[1](queries[:, 1])\n",
    "        rel_no_time = self.embeddings[3](queries[:, 1])\n",
    "        rhs = self.embeddings[0](queries[:, 2])\n",
    "        time = self.embeddings[2](queries[:, 3])\n",
    "        \n",
    "        input_sequence = torch.cat([lhs, rel, rhs,time], dim=0)  \n",
    "        # 将左实体、关系、右实体拼接成一个序列\n",
    "        attended_output, _ = self.self_attention(input_sequence, input_sequence, input_sequence)\n",
    "        \n",
    "        lhs, rel, rhs,time = torch.split(attended_output, [lhs.size(0), rel.size(0), rhs.size(0),time.size(0)], dim=0)\n",
    "        \n",
    "        \n",
    "\n",
    "        lhs = lhs[:, :self.rank], lhs[:, self.rank:]\n",
    "        rel = rel[:, :self.rank], rel[:, self.rank:]\n",
    "        time = time[:, :self.rank], time[:, self.rank:]\n",
    "        rnt = rel_no_time[:, :self.rank], rel_no_time[:, self.rank:]\n",
    "\n",
    "        rt = rel[0] * time[0], rel[1] * time[0], rel[0] * time[1], rel[1] * time[1]\n",
    "        full_rel = (rt[0] - rt[3]) + rnt[0], (rt[1] + rt[2]) + rnt[1]\n",
    "\n",
    "        h_static = self.static_embeddings[0](queries[:, 0])\n",
    "        r_static = self.static_embeddings[1](queries[:, 1])\n",
    "        t_static = self.static_embeddings[0](queries[:, 2])\n",
    "        \n",
    "        input_sequence_static = torch.cat([h_static, r_static, t_static], dim=0)  \n",
    "        # 将左实体、关系、右实体拼接成一个序列\n",
    "        attended_output_static, _ = self.self_attention_s(input_sequence_static, input_sequence_static, input_sequence_static)\n",
    "        \n",
    "        h_static, r_static, t_static = torch.split(attended_output_static, [h_static.size(0), r_static.size(0), t_static.size(0)], dim=0)\n",
    "\n",
    "        \n",
    "        h_static = h_static[:, :self.rank_static], h_static[:, self.rank_static:]\n",
    "        r_static = r_static[:, :self.rank_static], r_static[:, self.rank_static:]\n",
    "\n",
    "        return torch.cat([\n",
    "            lhs[0] * full_rel[0] - lhs[1] * full_rel[1],\n",
    "            lhs[1] * full_rel[0] + lhs[0] * full_rel[1]\n",
    "        ], 1), torch.cat([\n",
    "            h_static[0] * r_static[0] - h_static[1] * r_static[1],\n",
    "            h_static[1] * r_static[0] + h_static[0] * r_static[1]\n",
    "        ], 1)\n"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2023-12-07T07:58:32.174839Z",
     "iopub.execute_input": "2023-12-07T07:58:32.175226Z",
     "iopub.status.idle": "2023-12-07T07:58:35.358475Z",
     "shell.execute_reply.started": "2023-12-07T07:58:32.175179Z",
     "shell.execute_reply": "2023-12-07T07:58:35.357519Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Regularizers_rule"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Copyright (c) Facebook, Inc. and its affiliates.\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Regularizer(nn.Module, ABC):\n",
    "    @abstractmethod\n",
    "    def forward(self, factors: Tuple[torch.Tensor]):\n",
    "        pass\n",
    "\n",
    "class N3(Regularizer):\n",
    "    def __init__(self, weight: float):\n",
    "        super(N3, self).__init__()\n",
    "        self.weight = weight\n",
    "\n",
    "    def forward(self, factors):\n",
    "        norm = 0\n",
    "        for f in factors:\n",
    "            norm += self.weight * torch.sum(torch.abs(f)**3)\n",
    "        return norm / factors[0].shape[0]\n",
    "\n",
    "\n",
    "class Lambda3(Regularizer):\n",
    "    def __init__(self, weight: float):\n",
    "        super(Lambda3, self).__init__()\n",
    "        self.weight = weight\n",
    "\n",
    "    def forward(self, factor):\n",
    "        ddiff = factor[1:] - factor[:-1]\n",
    "        rank = int(ddiff.shape[1] / 2)\n",
    "        diff = torch.sqrt(ddiff[:, :rank]**2 + ddiff[:, rank:]**2)**3\n",
    "        return self.weight * torch.sum(diff) / (factor.shape[0] - 1)\n",
    "\n",
    "\n",
    "class RuleSim(Regularizer):\n",
    "    def __init__(self, weight: float):\n",
    "        super(RuleSim, self).__init__()\n",
    "        self.weight = weight\n",
    "\n",
    "    def forward(self, factors):\n",
    "        norm = self.weight * factors\n",
    "        return norm"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-07T07:58:35.359606Z",
     "iopub.execute_input": "2023-12-07T07:58:35.359969Z",
     "iopub.status.idle": "2023-12-07T07:58:35.370568Z",
     "shell.execute_reply.started": "2023-12-07T07:58:35.359944Z",
     "shell.execute_reply": "2023-12-07T07:58:35.369727Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "current_file_path = os.getcwd()\n",
    "current_file_path"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-07T07:58:35.371995Z",
     "iopub.execute_input": "2023-12-07T07:58:35.372331Z",
     "iopub.status.idle": "2023-12-07T07:58:35.385809Z",
     "shell.execute_reply.started": "2023-12-07T07:58:35.372300Z",
     "shell.execute_reply": "2023-12-07T07:58:35.384948Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Datasets_lcge"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "# 获取当前文件的路径\n",
    "current_file_path = os.getcwd()\n",
    "\n",
    "# 构建数据文件夹路径\n",
    "DATA_PATH = os.path.join(current_file_path, 'data')\n",
    "\n",
    "\n",
    "class TemporalDataset(object):\n",
    "    def __init__(self, name: str):\n",
    "        self.root = Path(DATA_PATH) / name\n",
    "        # 数据，存到字典，值为四元组的id\n",
    "        self.data = {}\n",
    "        for f in ['train', 'test', 'valid']:\n",
    "            in_file = open(str(self.root / (f + '.pickle')), 'rb')\n",
    "            self.data[f] = pickle.load(in_file)\n",
    "\n",
    "        maxis = np.max(self.data['train'], axis=0)\n",
    "        self.n_entities = int(max(maxis[0], maxis[2]) + 1)  # 实体数为主宾语相加\n",
    "        self.n_predicates = int(maxis[1] + 1)\n",
    "        self.n_predicates *= 2  # 谓语动词为双向，有部分关系是可逆的\n",
    "        if maxis.shape[0] > 4:  # 如果为5维，是有时间头，有时间尾，四维则不是\n",
    "            self.n_timestamps = max(int(maxis[3] + 1), int(maxis[4] + 1))\n",
    "        else:\n",
    "            self.n_timestamps = int(maxis[3] + 1)\n",
    "            \n",
    "        try:\n",
    "            # 检测时间戳是否规则，wikidata 数据集不规则，需要二次处理\n",
    "            inp_f = open(str(self.root / f'ts_diffs.pickle'), 'rb')\n",
    "            self.time_diffs = torch.from_numpy(pickle.load(inp_f)).cuda().float()\n",
    "            # print(\"Assume all timestamps are regularly spaced\")\n",
    "            # self.time_diffs = None\n",
    "            inp_f.close()\n",
    "        except OSError:\n",
    "            print(\"Assume all timestamps are regularly spaced\")\n",
    "            self.time_diffs = None\n",
    "\n",
    "        try:\n",
    "            # 检测数据集是否含有event—list，如果没有不使用时间间隔和事件评估\n",
    "            e = open(str(self.root / f'event_list_all.pickle'), 'rb')\n",
    "            self.events = pickle.load(e)\n",
    "            e.close()\n",
    "\n",
    "            f = open(str(self.root / f'ts_id'), 'rb')\n",
    "            dictionary = pickle.load(f)\n",
    "            f.close()\n",
    "            self.timestamps = sorted(dictionary.keys())\n",
    "        except OSError:\n",
    "            print(\"Not using time intervals and events eval\")\n",
    "            self.events = None\n",
    "\n",
    "        if self.events is None:\n",
    "            # 如果无evert—list，使用to—skip中存储的边，将主谓语进行链接\n",
    "            inp_f = open(str(self.root / f'to_skip.pickle'), 'rb')\n",
    "            self.to_skip: Dict[str, Dict[Tuple[int, int, int], List[int]]] = pickle.load(inp_f)\n",
    "            inp_f.close()\n",
    "\n",
    "\n",
    "\n",
    "        # If dataset has events, it's wikidata.\n",
    "        # For any relation that has no beginning & no end:\n",
    "        # add special beginning = end = no_timestamp, increase n_timestamps by one.\n",
    "\n",
    "    def has_intervals(self):\n",
    "        return self.events is not None\n",
    "\n",
    "    def get_examples(self, split):\n",
    "        # split为字符串，test，train，valid\n",
    "        return self.data[split]\n",
    "\n",
    "    def get_train(self):\n",
    "        # 主宾语换位置，边反向，返回值为正向反向堆叠在一块，相当于把图谱所有边拿出来\n",
    "        copy = np.copy(self.data['train'])\n",
    "        print(\"\\nexamples:\\n\", copy.shape)\n",
    "        tmp = np.copy(copy[:, 0])\n",
    "        copy[:, 0] = copy[:, 2]\n",
    "        copy[:, 2] = tmp\n",
    "        copy[:, 1] += self.n_predicates // 2  # has been multiplied by two.\n",
    "        return np.vstack((self.data['train'], copy))\n",
    "\n",
    "    def eval(\n",
    "            self, model: TKBCModel, split: str, n_queries: int = -1, missing_eval: str = 'both',\n",
    "            at: Tuple[int] = (1, 3, 10)\n",
    "    ):\n",
    "        if self.events is not None:\n",
    "            return self.time_eval(model, split, n_queries, 'rhs', at)\n",
    "        test = self.get_examples(split)\n",
    "        examples = torch.from_numpy(test.astype('int64')).cuda()\n",
    "        missing = [missing_eval]\n",
    "        if missing_eval == 'both':\n",
    "            missing = ['rhs', 'lhs']\n",
    "\n",
    "        mean_reciprocal_rank = {}\n",
    "        hits_at = {}\n",
    "\n",
    "        for m in missing:\n",
    "            q = examples.clone()\n",
    "            if n_queries > 0:\n",
    "                permutation = torch.randperm(len(examples))[:n_queries]\n",
    "                q = examples[permutation]\n",
    "            if m == 'lhs':\n",
    "                tmp = torch.clone(q[:, 0])\n",
    "                q[:, 0] = q[:, 2]\n",
    "                q[:, 2] = tmp\n",
    "                q[:, 1] += self.n_predicates // 2\n",
    "            ranks = model.get_ranking(q, self.to_skip[m], batch_size=500)\n",
    "            mean_reciprocal_rank[m] = torch.mean(1. / ranks).item()\n",
    "            hits_at[m] = torch.FloatTensor((list(map(\n",
    "                lambda x: torch.mean((ranks <= x).float()).item(),\n",
    "                at\n",
    "            ))))\n",
    "\n",
    "        return mean_reciprocal_rank, hits_at\n",
    "\n",
    "    def time_eval(\n",
    "            self, model: TKBCModel, split: str, n_queries: int = -1, missing_eval: str = 'both',\n",
    "            at: Tuple[int] = (1, 3, 10)\n",
    "    ):\n",
    "        assert missing_eval == 'rhs', \"other evals not implemented\"\n",
    "        test = torch.from_numpy(\n",
    "            self.get_examples(split).astype('int64')\n",
    "        )\n",
    "        if n_queries > 0:\n",
    "            permutation = torch.randperm(len(test))[:n_queries]\n",
    "            test = test[permutation]\n",
    "\n",
    "        time_range = test.float()\n",
    "        sampled_time = (\n",
    "                torch.rand(time_range.shape[0]) * (time_range[:, 4] - time_range[:, 3]) + time_range[:, 3]\n",
    "        ).round().long()\n",
    "        has_end = (time_range[:, 4] != (self.n_timestamps - 1))\n",
    "        has_start = (time_range[:, 3] > 0)\n",
    "\n",
    "        masks = {\n",
    "            'full_time': has_end + has_start,\n",
    "            'only_begin': has_start * (~has_end),\n",
    "            'only_end': has_end * (~has_start),\n",
    "            'no_time': (~has_end) * (~has_start)\n",
    "        }\n",
    "\n",
    "        with_time = torch.cat((\n",
    "            sampled_time.unsqueeze(1),\n",
    "            time_range[:, 0:3].long(),\n",
    "            masks['full_time'].long().unsqueeze(1),\n",
    "            masks['only_begin'].long().unsqueeze(1),\n",
    "            masks['only_end'].long().unsqueeze(1),\n",
    "            masks['no_time'].long().unsqueeze(1),\n",
    "        ), 1)\n",
    "        # generate events\n",
    "        eval_events = sorted(with_time.tolist())\n",
    "\n",
    "        to_filter: Dict[Tuple[int, int], Dict[int, int]] = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "        id_event = 0\n",
    "        id_timeline = 0\n",
    "        batch_size = 100\n",
    "        to_filter_batch = []\n",
    "        cur_batch = []\n",
    "\n",
    "        ranks = {\n",
    "            'full_time': [], 'only_begin': [], 'only_end': [], 'no_time': [],\n",
    "            'all': []\n",
    "        }\n",
    "        while id_event < len(eval_events):\n",
    "            # Follow timeline to add events to filters\n",
    "            while id_timeline < len(self.events) and self.events[id_timeline][0] <= eval_events[id_event][3]:\n",
    "                date, event_type, (lhs, rel, rhs) = self.events[id_timeline]\n",
    "                if event_type < 0:  # begin\n",
    "                    to_filter[(lhs, rel)][rhs] += 1\n",
    "                if event_type > 0:  # end\n",
    "                    to_filter[(lhs, rel)][rhs] -= 1\n",
    "                    if to_filter[(lhs, rel)][rhs] == 0:\n",
    "                        del to_filter[(lhs, rel)][rhs]\n",
    "                id_timeline += 1\n",
    "            date, lhs, rel, rhs, full_time, only_begin, only_end, no_time = eval_events[id_event]\n",
    "\n",
    "            to_filter_batch.append(sorted(to_filter[(lhs, rel)].keys()))\n",
    "            cur_batch.append((lhs, rel, rhs, date, full_time, only_begin, only_end, no_time))\n",
    "            # once a batch is ready, call get_ranking and reset\n",
    "            if len(cur_batch) == batch_size or id_event == len(eval_events) - 1:\n",
    "                cuda_batch = torch.cuda.LongTensor(cur_batch)\n",
    "                bbatch = torch.LongTensor(cur_batch)\n",
    "                batch_ranks = model.get_time_ranking(cuda_batch[:, :4], to_filter_batch, 500000)\n",
    "\n",
    "                ranks['full_time'].append(batch_ranks[bbatch[:, 4] == 1])\n",
    "                ranks['only_begin'].append(batch_ranks[bbatch[:, 5] == 1])\n",
    "                ranks['only_end'].append(batch_ranks[bbatch[:, 6] == 1])\n",
    "                ranks['no_time'].append(batch_ranks[bbatch[:, 7] == 1])\n",
    "\n",
    "                ranks['all'].append(batch_ranks)\n",
    "                cur_batch = []\n",
    "                to_filter_batch = []\n",
    "            id_event += 1\n",
    "\n",
    "        ranks = {x: torch.cat(ranks[x]) for x in ranks if len(ranks[x]) > 0}\n",
    "        mean_reciprocal_rank = {x: torch.mean(1. / ranks[x]).item() for x in ranks if len(ranks[x]) > 0}\n",
    "        hits_at = {z: torch.FloatTensor((list(map(\n",
    "            lambda x: torch.mean((ranks[z] <= x).float()).item(),\n",
    "            at\n",
    "        )))) for z in ranks if len(ranks[z]) > 0}\n",
    "\n",
    "        res = {\n",
    "            ('MRR_'+x): y for x, y in mean_reciprocal_rank.items()\n",
    "        }\n",
    "        res.update({('hits@_'+x): y for x, y in hits_at.items()})\n",
    "        return res\n",
    "\n",
    "    def breakdown_time_eval(\n",
    "            self, model: TKBCModel, split: str, n_queries: int = -1, missing_eval: str = 'rhs',\n",
    "    ):\n",
    "        # 在ICEWS14中未使用\n",
    "        assert missing_eval == 'rhs', \"other evals not implemented\"\n",
    "        test = torch.from_numpy(\n",
    "            self.get_examples(split).astype('int64')\n",
    "        )\n",
    "        if n_queries > 0:\n",
    "            permutation = torch.randperm(len(test))[:n_queries]\n",
    "            test = test[permutation]\n",
    "\n",
    "        time_range = test.float()\n",
    "        sampled_time = (\n",
    "                torch.rand(time_range.shape[0]) * (time_range[:, 4] - time_range[:, 3]) + time_range[:, 3]\n",
    "        ).round().long()\n",
    "        has_end = (time_range[:, 4] != (self.n_timestamps - 1))\n",
    "        has_start = (time_range[:, 3] > 0)\n",
    "\n",
    "        masks = {\n",
    "            'full_time': has_end + has_start,\n",
    "            'only_begin': has_start * (~has_end),\n",
    "            'only_end': has_end * (~has_start),\n",
    "            'no_time': (~has_end) * (~has_start)\n",
    "        }\n",
    "\n",
    "        with_time = torch.cat((\n",
    "            sampled_time.unsqueeze(1),\n",
    "            time_range[:, 0:3].long(),\n",
    "            masks['full_time'].long().unsqueeze(1),\n",
    "            masks['only_begin'].long().unsqueeze(1),\n",
    "            masks['only_end'].long().unsqueeze(1),\n",
    "            masks['no_time'].long().unsqueeze(1),\n",
    "        ), 1)\n",
    "        # generate events\n",
    "        eval_events = sorted(with_time.tolist())\n",
    "\n",
    "        to_filter: Dict[Tuple[int, int], Dict[int, int]] = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "        id_event = 0\n",
    "        id_timeline = 0\n",
    "        batch_size = 100\n",
    "        to_filter_batch = []\n",
    "        cur_batch = []\n",
    "\n",
    "        ranks = defaultdict(list)\n",
    "        while id_event < len(eval_events):\n",
    "            # Follow timeline to add events to filters\n",
    "            while id_timeline < len(self.events) and self.events[id_timeline][0] <= eval_events[id_event][3]:\n",
    "                date, event_type, (lhs, rel, rhs) = self.events[id_timeline]\n",
    "                if event_type < 0:  # begin\n",
    "                    to_filter[(lhs, rel)][rhs] += 1\n",
    "                if event_type > 0:  # end\n",
    "                    to_filter[(lhs, rel)][rhs] -= 1\n",
    "                    if to_filter[(lhs, rel)][rhs] == 0:\n",
    "                        del to_filter[(lhs, rel)][rhs]\n",
    "                id_timeline += 1\n",
    "            date, lhs, rel, rhs, full_time, only_begin, only_end, no_time = eval_events[id_event]\n",
    "\n",
    "            to_filter_batch.append(sorted(to_filter[(lhs, rel)].keys()))\n",
    "            cur_batch.append((lhs, rel, rhs, date, full_time, only_begin, only_end, no_time))\n",
    "            # once a batch is ready, call get_ranking and reset\n",
    "            if len(cur_batch) == batch_size or id_event == len(eval_events) - 1:\n",
    "                cuda_batch = torch.cuda.LongTensor(cur_batch)\n",
    "                bbatch = torch.LongTensor(cur_batch)\n",
    "                batch_ranks = model.get_time_ranking(cuda_batch[:, :4], to_filter_batch, 500000)\n",
    "                for rank, predicate in zip(batch_ranks, bbatch[:, 1]):\n",
    "                    ranks[predicate.item()].append(rank.item())\n",
    "                cur_batch = []\n",
    "                to_filter_batch = []\n",
    "            id_event += 1\n",
    "\n",
    "        ranks = {x: torch.FloatTensor(ranks[x]) for x in ranks}\n",
    "        sum_reciprocal_rank = {x: torch.sum(1. / ranks[x]).item() for x in ranks}\n",
    "\n",
    "        return sum_reciprocal_rank\n",
    "\n",
    "    def time_AUC(self, model: TKBCModel, split: str, n_queries: int = -1):\n",
    "        # AUC面积值越接近1，模型正确率越高，0.5等同于随机猜测\n",
    "        test = torch.from_numpy(\n",
    "            self.get_examples(split).astype('int64')\n",
    "        )\n",
    "        if n_queries > 0:\n",
    "            permutation = torch.randperm(len(test))[:n_queries]\n",
    "            test = test[permutation]\n",
    "\n",
    "        truth, scores = model.get_auc(test.cuda())\n",
    "\n",
    "        return {\n",
    "            'micro': average_precision_score(truth, scores, average='micro'),\n",
    "            'macro': average_precision_score(truth, scores, average='macro')\n",
    "        }\n",
    "\n",
    "\n",
    "    def get_shape(self):\n",
    "        # 获取图谱四元组的尺寸，不是tensor的shape\n",
    "        return self.n_entities, self.n_predicates, self.n_entities, self.n_timestamps\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-07T07:58:35.386932Z",
     "iopub.execute_input": "2023-12-07T07:58:35.387176Z",
     "iopub.status.idle": "2023-12-07T07:58:35.914572Z",
     "shell.execute_reply.started": "2023-12-07T07:58:35.387154Z",
     "shell.execute_reply": "2023-12-07T07:58:35.913750Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Optimizers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Copyright (c) Facebook, Inc. and its affiliates.\n",
    "\n",
    "import torch\n",
    "import tqdm\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torchviz import make_dot\n",
    "\n",
    "\n",
    "class TKBCOptimizer(object):\n",
    "    def __init__(\n",
    "            self, model: TKBCModel,\n",
    "            emb_regularizer: Regularizer, temporal_regularizer: Regularizer, rule_regularizer: Regularizer,\n",
    "            optimizer: optim.Optimizer, batch_size: int = 256,\n",
    "            verbose: bool = True\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.emb_regularizer = emb_regularizer\n",
    "        self.temporal_regularizer = temporal_regularizer\n",
    "        self.optimizer = optimizer\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "        self.rule_regularizer = rule_regularizer\n",
    "        \n",
    "\n",
    "        self.run_once = True\n",
    "\n",
    "    def epoch(self, examples: torch.LongTensor):\n",
    "        actual_examples = examples[torch.randperm(examples.shape[0]), :]\n",
    "        loss = nn.CrossEntropyLoss(reduction='mean')\n",
    "        loss_static = nn.CrossEntropyLoss(reduction='mean')\n",
    "        with tqdm.tqdm(total=examples.shape[0], unit='ex', disable=not self.verbose) as bar:\n",
    "            bar.set_description(f'train loss')\n",
    "            b_begin = 0\n",
    "            while b_begin < examples.shape[0]:\n",
    "                input_batch = actual_examples[\n",
    "                              b_begin:b_begin + self.batch_size\n",
    "                              ].cuda()\n",
    "                predictions, pred_static, factors, time, rule = self.model.forward(input_batch)\n",
    "                truth = input_batch[:, 2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                if self.run_once:\n",
    "                    self.run_once = False\n",
    "                    print(\"prediction.shape:\",predictions.shape)\n",
    "                    print(\"truth.shape:\",truth.shape)\n",
    "                    # 对 logits 应用 softmax 函数\n",
    "                    probs = F.softmax(predictions, dim=0)\n",
    "                    print(probs.shape)\n",
    "                    max_prob, max_index = torch.max(probs, dim=1)\n",
    "                    # 找到概率最大的值和对应的下标\n",
    "#                     print(max_prob[:30],\"\\n\", max_index[:30],\"\\n\",truth[:30])\n",
    "                    # 计算预测的类别与真实标签相同的数量\n",
    "                    num_correct = (max_index == truth).sum().item()\n",
    "                    # 计算总样本数量\n",
    "                    total_samples = len(truth)\n",
    "                    # 计算正确率\n",
    "                    accuracy = num_correct / total_samples\n",
    "                    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "                \n",
    "                \n",
    "                \n",
    "                    \n",
    "                l_fit = loss(predictions, truth)\n",
    "                l_static = loss_static(pred_static, truth)\n",
    "                l_reg = self.emb_regularizer.forward(factors)\n",
    "                l_time = torch.zeros_like(l_reg)\n",
    "                l_rule = self.rule_regularizer.forward(rule)\n",
    "                if time is not None:\n",
    "                    l_time = self.temporal_regularizer.forward(time)\n",
    "                l = l_fit + 0.1 * l_static + l_reg + l_time\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                l.backward()\n",
    "                self.optimizer.step()\n",
    "                b_begin += self.batch_size\n",
    "                bar.update(input_batch.shape[0])\n",
    "                bar.set_postfix(\n",
    "                    loss=f'{l_fit.item():.5f}',\n",
    "                    loss_cs=f'{l_static.item():.5f}',\n",
    "                    reg=f'{l_reg.item():.5f}',\n",
    "                    cont=f'{l_time.item():.5f}',\n",
    "                    rule=f'{l_rule.item():.5f}'\n",
    "                )\n",
    "                # if b_begin == 0:\n",
    "                #     output = self.model.forward(input_batch)\n",
    "                #     make_dot(output[0], params=dict(self.model.named_parameters())).render(\"predictions_compute_graph\",\n",
    "                #                                                                            format=\"png\")\n",
    "                #     make_dot(output[1], params=dict(self.model.named_parameters())).render(\"pred_static_compute_graph\",\n",
    "                #                                                                            format=\"png\")\n",
    "                #     print('drawing done')\n",
    "\n",
    "\n",
    "class IKBCOptimizer(object):\n",
    "    def __init__(\n",
    "            self, model: TKBCModel,\n",
    "            emb_regularizer: Regularizer, temporal_regularizer: Regularizer,\n",
    "            optimizer: optim.Optimizer, dataset: TemporalDataset, batch_size: int = 256,\n",
    "            verbose: bool = True\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.dataset = dataset\n",
    "        self.emb_regularizer = emb_regularizer\n",
    "        self.temporal_regularizer = temporal_regularizer\n",
    "        self.optimizer = optimizer\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def epoch(self, examples: torch.LongTensor):\n",
    "        actual_examples = examples[torch.randperm(examples.shape[0]), :]\n",
    "        loss = nn.CrossEntropyLoss(reduction='mean')\n",
    "        with tqdm.tqdm(total=examples.shape[0], unit='ex', disable=not self.verbose) as bar:\n",
    "            bar.set_description(f'train loss')\n",
    "            b_begin = 0\n",
    "            while b_begin < examples.shape[0]:\n",
    "                time_range = actual_examples[b_begin:b_begin + self.batch_size].cuda()\n",
    "\n",
    "                ## RHS Prediction loss\n",
    "                sampled_time = (\n",
    "                        torch.rand(time_range.shape[0]).cuda() * (time_range[:, 4] - time_range[:, 3]).float() +\n",
    "                        time_range[:, 3].float()\n",
    "                ).round().long()\n",
    "                with_time = torch.cat((time_range[:, 0:3], sampled_time.unsqueeze(1)), 1)\n",
    "\n",
    "                predictions, factors, time = self.model.forward(with_time)\n",
    "                truth = with_time[:, 2]\n",
    "\n",
    "                l_fit = loss(predictions, truth)\n",
    "\n",
    "                ## Time prediction loss (ie cross entropy over time)\n",
    "                time_loss = 0.\n",
    "                if self.model.has_time():\n",
    "                    filtering = ~(\n",
    "                            (time_range[:, 3] == 0) *\n",
    "                            (time_range[:, 4] == (self.dataset.n_timestamps - 1))\n",
    "                    )  # NOT no begin and no end\n",
    "                    these_examples = time_range[filtering, :]\n",
    "                    truth = (\n",
    "                            torch.rand(these_examples.shape[0]).cuda() * (\n",
    "                            these_examples[:, 4] - these_examples[:, 3]).float() +\n",
    "                            these_examples[:, 3].float()\n",
    "                    ).round().long()\n",
    "                    time_predictions = self.model.forward_over_time(these_examples[:, :3].cuda().long())\n",
    "                    time_loss = loss(time_predictions, truth.cuda())\n",
    "\n",
    "                l_reg = self.emb_regularizer.forward(factors)\n",
    "                l_time = torch.zeros_like(l_reg)\n",
    "                if time is not None:\n",
    "                    l_time = self.temporal_regularizer.forward(time)\n",
    "                l = l_fit + l_reg + l_time + time_loss\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                l.backward()\n",
    "                self.optimizer.step()\n",
    "                b_begin += self.batch_size\n",
    "                bar.update(with_time.shape[0])\n",
    "                bar.set_postfix(\n",
    "                    loss=f'{l_fit.item():.0f}',\n",
    "                    loss_time=f'{time_loss if type(time_loss) == float else time_loss.item() :.0f}',\n",
    "                    reg=f'{l_reg.item():.0f}',\n",
    "                    cont=f'{l_time.item():.4f}'\n",
    "                )\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-07T07:58:35.915940Z",
     "iopub.execute_input": "2023-12-07T07:58:35.916274Z",
     "iopub.status.idle": "2023-12-07T07:58:36.033563Z",
     "shell.execute_reply.started": "2023-12-07T07:58:35.916246Z",
     "shell.execute_reply": "2023-12-07T07:58:36.032728Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Learner_lcge"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import argparse\n",
    "import json\n",
    "from typing import Dict\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# 命令行输入参数\n",
    "# python learner_lcge.py --dataset ICEWS14 --model LCGE --rank(分解等级) 2000\n",
    "# --emb_reg(嵌入正则化强度) 0.005 --time_reg(时间戳正则化强度) 0.01 --rule_reg(规则正则化强度) 0.01 --max_epoch 1000\n",
    "# --weight_static(静态分数的权重) 0.1 --learning_rate(学习率) 0.1\n",
    "\n",
    "parser = argparse.ArgumentParser(\n",
    "    description=\"Logic and Commonsense-Guided Temporal KGE\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--dataset', type=str, default=\"ICEWS14\",\n",
    "    help=\"Dataset name\"\n",
    ")\n",
    "models = [\n",
    "    'LCGE'\n",
    "]\n",
    "parser.add_argument(\n",
    "    '--model', choices=models,default=\"LCGE\",\n",
    "    help=\"Model in {}\".format(models)\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--max_epochs', default=200, type=int,\n",
    "    help=\"Number of epochs.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--valid_freq', default=2, type=int,\n",
    "    help=\"Number of epochs between each valid.\"\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    '--rank', default=2000, type=int,\n",
    "    help=\"Factorization rank.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--batch_size', default=1000, type=int,\n",
    "    help=\"Batch size.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--learning_rate', default=0.1, type=float,\n",
    "    help=\"Learning rate\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--emb_reg', default=0.000005, type=float,\n",
    "    help=\"Embedding regularizer strength\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--time_reg', default=0.01, type=float,\n",
    "    help=\"Timestamp regularizer strength\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--no_time_emb', default=False, action=\"store_true\",\n",
    "    help=\"Use a specific embedding for non temporal relations\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--rule_reg', default=0.01, type=float,\n",
    "    help=\"Rule regularizer strength\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--weight_static', default=0.1, type=float,\n",
    "    help=\"Weight of static score\"\n",
    ")\n",
    "\n",
    "args = vars(parser.parse_args([])) \n",
    "\n",
    "\n",
    "print(\"默认参数：\", args)\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-07T07:58:36.034817Z",
     "iopub.execute_input": "2023-12-07T07:58:36.035437Z",
     "iopub.status.idle": "2023-12-07T07:58:47.531468Z",
     "shell.execute_reply.started": "2023-12-07T07:58:36.035401Z",
     "shell.execute_reply": "2023-12-07T07:58:47.530502Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "list_args=[]\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-07T07:58:47.533979Z",
     "iopub.execute_input": "2023-12-07T07:58:47.534565Z",
     "iopub.status.idle": "2023-12-07T07:58:47.538968Z",
     "shell.execute_reply.started": "2023-12-07T07:58:47.534535Z",
     "shell.execute_reply": "2023-12-07T07:58:47.537907Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "list_args.append(args)\n",
    "print(list_args)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-07T07:58:47.540542Z",
     "iopub.execute_input": "2023-12-07T07:58:47.540979Z",
     "iopub.status.idle": "2023-12-07T07:58:47.557386Z",
     "shell.execute_reply.started": "2023-12-07T07:58:47.540953Z",
     "shell.execute_reply": "2023-12-07T07:58:47.556423Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "args['dataset']"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-07T07:58:47.558606Z",
     "iopub.execute_input": "2023-12-07T07:58:47.558922Z",
     "iopub.status.idle": "2023-12-07T07:58:47.568048Z",
     "shell.execute_reply.started": "2023-12-07T07:58:47.558897Z",
     "shell.execute_reply": "2023-12-07T07:58:47.567120Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "dataset = TemporalDataset(args[\"dataset\"])\n",
    "\n",
    "with open(current_file_path+\"/src_data/rulelearning/\" + args[\"dataset\"] + \"/rule1_p1.json\", 'r') as load_rule1_p1:\n",
    "    rule1_p1 = json.load(load_rule1_p1)\n",
    "with open(\"./src_data/rulelearning/\" + args[\"dataset\"] + \"/rule1_p2.json\", 'r') as load_rule1_p2:\n",
    "    rule1_p2 = json.load(load_rule1_p2)\n",
    "\n",
    "f = open(\"./src_data/rulelearning/\" + args[\"dataset\"] + \"/rule2_p1.txt\", 'r')\n",
    "rule2_p1 = {}\n",
    "for line in f:\n",
    "    head, body1, body2, confi = line.strip().split(\"\\t\")\n",
    "    head, body1, body2, confi = int(head), int(body1), int(body2), float(confi)\n",
    "    if head not in rule2_p1:\n",
    "        rule2_p1[head] = {}\n",
    "    rule2_p1[head][(body1, body2)] = confi\n",
    "f.close()\n",
    "\n",
    "f = open(\"./src_data/rulelearning/\" + args[\"dataset\"] + \"/rule2_p2.txt\", 'r')\n",
    "rule2_p2 = {}\n",
    "for line in f:\n",
    "    head, body1, body2, confi = line.strip().split(\"\\t\")\n",
    "    head, body1, body2, confi = int(head), int(body1), int(body2), float(confi)\n",
    "    if head not in rule2_p2:\n",
    "        rule2_p2[head] = {}\n",
    "    rule2_p2[head][(body1, body2)] = confi\n",
    "f.close()\n",
    "\n",
    "f = open(\"./src_data/rulelearning/\" + args[\"dataset\"] + \"/rule2_p3.txt\", 'r')\n",
    "rule2_p3 = {}\n",
    "for line in f:\n",
    "    head, body1, body2, confi = line.strip().split(\"\\t\")\n",
    "    head, body1, body2, confi = int(head), int(body1), int(body2), float(confi)\n",
    "    if head not in rule2_p3:\n",
    "        rule2_p3[head] = {}\n",
    "    rule2_p3[head][(body1, body2)] = confi\n",
    "f.close()\n",
    "\n",
    "f = open(\"./src_data/rulelearning/\" + args[\"dataset\"] + \"/rule2_p4.txt\", 'r')\n",
    "rule2_p4 = {}\n",
    "for line in f:\n",
    "    head, body1, body2, confi = line.strip().split(\"\\t\")\n",
    "    head, body1, body2, confi = int(head), int(body1), int(body2), float(confi)\n",
    "    if head not in rule2_p4:\n",
    "        rule2_p4[head] = {}\n",
    "    rule2_p4[head][(body1, body2)] = confi\n",
    "f.close()\n",
    "\n",
    "rules = (rule1_p1, rule1_p2, rule2_p1, rule2_p2, rule2_p3, rule2_p4)\n",
    "\n",
    "sizes = dataset.get_shape()\n",
    "print(\"sizes of dataset is:\\t\", sizes)\n",
    "model = {\n",
    "    'LCGE': LCGE(sizes, args[\"rank\"], rules, args['weight_static'], no_time_emb=args['no_time_emb']),\n",
    "}[args['model']]\n",
    "model = model.cuda()\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#     print(f\"使用 {torch.cuda.device_count()} 个 GPU.\")\n",
    "#     model = nn.DataParallel(model)\n",
    "\n",
    "opt = optim.Adagrad(model.parameters(), lr=args['learning_rate'])\n",
    "\n",
    "emb_reg = N3(args['emb_reg'])\n",
    "time_reg = Lambda3(args['time_reg'])\n",
    "rule_reg = RuleSim(args['rule_reg'])  # relation embedding reglu via rules\n",
    "\n",
    "best_mrr = 0.\n",
    "best_hit = 0.\n",
    "early_stopping = 0\n",
    "writer = SummaryWriter(log_dir='logs')\n",
    "for args in list_args:\n",
    "    for epoch in range(args['max_epochs']):\n",
    "        examples = torch.from_numpy(\n",
    "            dataset.get_train().astype('int64')\n",
    "        )\n",
    "        print(\"\\nexamples:\\n\", examples.size())\n",
    "\n",
    "        model.train()\n",
    "        if dataset.has_intervals():\n",
    "            optimizer = IKBCOptimizer(\n",
    "                model, emb_reg, time_reg, opt, dataset,\n",
    "                batch_size=args.batch_size\n",
    "            )\n",
    "            optimizer.epoch(examples)\n",
    "\n",
    "        else:\n",
    "            optimizer = TKBCOptimizer(\n",
    "                model, emb_reg, time_reg, rule_reg, opt,\n",
    "                batch_size=args['batch_size']\n",
    "            )\n",
    "            optimizer.epoch(examples)\n",
    "\n",
    "\n",
    "        def avg_both(mrrs: Dict[str, float], hits: Dict[str, torch.FloatTensor]):\n",
    "            \"\"\"\n",
    "            aggregate metrics for missing lhs and rhs\n",
    "            :param mrrs: d\n",
    "            :param hits:\n",
    "            :return:\n",
    "            \"\"\"\n",
    "            m = (mrrs['lhs'] + mrrs['rhs']) / 2.\n",
    "            h = (hits['lhs'] + hits['rhs']) / 2.\n",
    "            return {'MRR': m, 'hits@[1,3,10]': h}\n",
    "\n",
    "\n",
    "        if epoch < 0 or (epoch + 1) % args['valid_freq'] == 0:\n",
    "            if dataset.has_intervals():\n",
    "                valid, test, train = [\n",
    "                    dataset.eval(model, split, -1 if split != 'train' else 50000)\n",
    "                    for split in ['valid', 'test', 'train']\n",
    "                ]\n",
    "                print(\"valid: \", valid)\n",
    "                print(\"test: \", test)\n",
    "                print(\"train: \", train)\n",
    "\n",
    "            else:\n",
    "                valid, test, train = [\n",
    "                    avg_both(*dataset.eval(model, split, -1 if split != 'train' else 50000))\n",
    "                    for split in ['valid', 'test', 'train']\n",
    "                ]\n",
    "                print(\"epoch: \", epoch + 1)\n",
    "                print(\"valid: \", valid['MRR'])\n",
    "                print(\"test: \", test['MRR'])\n",
    "                print(\"train: \", train['MRR'])\n",
    "\n",
    "                writer.add_scalar('valid', valid['MRR'], epoch)\n",
    "                writer.add_scalar('test', test['MRR'], epoch)\n",
    "                writer.add_scalar('train', train['MRR'], epoch)\n",
    "\n",
    "                writer.add_scalar('hit@1', test['hits@[1,3,10]'][0], epoch)\n",
    "                writer.add_scalar('hit@3', test['hits@[1,3,10]'][1], epoch)\n",
    "                writer.add_scalar('hit@10', test['hits@[1,3,10]'][2], epoch)\n",
    "\n",
    "                print(\"test hits@n:\\t\", test['hits@[1,3,10]'])\n",
    "                if test['MRR'] > best_mrr:\n",
    "                    best_mrr = test['MRR']\n",
    "                    best_hit = test['hits@[1,3,10]']\n",
    "                    early_stopping = 0\n",
    "                else:\n",
    "                    early_stopping += 1\n",
    "                if early_stopping > 5:\n",
    "                    print(\"early stopping!\")\n",
    "                    break\n",
    "\n",
    "    print(\"The best test mrr is:\\t\", best_mrr)\n",
    "    print(\"The best test hits@1,3,10 are:\\t\", best_hit)\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-07T07:58:47.569579Z",
     "iopub.execute_input": "2023-12-07T07:58:47.569946Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
